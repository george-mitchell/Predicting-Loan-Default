{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Loan Default 3 - Models\n",
    "\n",
    "In this notebook we perform model fitting, evaluation and selection. \n",
    "\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data handling \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import polars as pl \n",
    "import polars.selectors as cs\n",
    "\n",
    "## tuning\n",
    "import optuna \n",
    "\n",
    "## visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "\n",
    "## sklearn models\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## pickle \n",
    "from pickle import dump, load\n",
    "\n",
    "## get file path of the data\n",
    "from private import ENCODED_FILE_PATH, FINAL_FILE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load datasets\n",
    "X_train = pd.read_csv(FINAL_FILE_PATH + \"X_train.csv\", index_col=0)\n",
    "X_test = pd.read_csv(FINAL_FILE_PATH + \"X_test.csv\", index_col=0)\n",
    "y_train = pd.read_csv(FINAL_FILE_PATH + \"y_train.csv\", index_col=0)\n",
    "y_test = pd.read_csv(FINAL_FILE_PATH + \"y_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation \n",
    "\n",
    "Due to the imbalanced nature of our `default` feature, we must carefully consider our metric. As false negatives (borrowers who default but who were predicted to not default) are much more costly to investors than false positives (borrowers who don't default but who were predicted to default), we will make use of recall as a metric, as well as the area under the ROC curve. \n",
    "\n",
    "### Model Selection \n",
    "\n",
    "We will use 3-fold cross validation on a large number of models with default parameters. This will help gauge which type of model may be performing best. We will then select the best performing model for hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "## baseline models \n",
    "models.append(('LR', LogisticRegression(max_iter=10000)))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "## ensemble models \n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "## boosting models \n",
    "models.append(('GBM', GradientBoostingClassifier()))\n",
    "models.append(('AB', AdaBoostClassifier(algorithm=\"SAMME\")))\n",
    "## neural networks\n",
    "models.append(('NN', MLPClassifier()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.8734 (0.000921)\n",
      "KNN: 0.83 (0.000809)\n",
      "CART: 0.8754 (0.00106)\n",
      "NB: 0.822 (0.001115)\n",
      "RF: 0.8961 (0.000562)\n",
      "GBM: 0.8958 (0.000408)\n",
      "AB: 0.8786 (0.001366)\n",
      "NN: 0.8954 (0.001303)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5)\n",
    "for name, model in models :\n",
    "    cv_results = cross_val_score(model, \n",
    "                                 X_train, \n",
    "                                 y_train.values.ravel(),\n",
    "                                 cv=kfold, \n",
    "                                 scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(f\"{name}: {round(cv_results.mean(), 4)} ({round(cv_results.std(), 6)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results so they can be loaded later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = {name:result for (name,result) in zip(names,results)}\n",
    "dump(cv_scores, open(\"../models/cv_scores.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = load(open(\"../models/cv_scores.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the results for each model below: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
